## Project: Build a Traffic Sign Recognition Program

The goal of this project is to build a program that can classify German traffic signs using a neural network.  
To archive this goal, the data must be loaded first. After importing all necessary libraries such as pickle, sklearn, numpy, cv2, pandas and many more, I defined the path for the training-, validation- and testing data. Using pickle the data was extracted from the .p files.  
Afterwards a brief exploration of the dataset was given. For this, the number of training-, validation and testing-examples was extracted by checking the length of each dataset. By calling X_train[0].shape the image shape of a training image can be displayed. The basic summary also shows, how many different classes (different German traffic signs) there are inside the training set. To get this number, the numpy function np.unique() was applied in combination with len() to the dataset y_train, which contains the labels for every image inside X_train.  
To give an exploratory visualisation of the dataset, I first created a random number called “index” by using the random.randint() function and giving 0 and the length of the X_train dataset as input parameters. Afterwards I created the image by taking the data from the X_train dataset with the random number created earlier as an index. Squeeze() helped to remove a dimension. The random training image is then displayed with the plt.imshow() command, along with the printed image label (y_train with the random number as an index) an example is shown in image 1. It can be seen that the image is classified as number 5. By looking into the signnames.csv file we can see that the random image is a speed limit (80km/h) which is right.

Afterwards I wanted to look at how many images for each class are inside the dataset. For this I again used the np.unique() function with y_train as an input and with return_counts set to ‘true’. Then I printed the results. The distribution of pictures can be seen in image 2. The y-axis represents the number of images for the class and the x-axis shows the class number. It can be seen that the number of images is not the same for each class. Some classes have more than or close to 2000 images while others have 200 or less. This difference could limit the performance of the network.

My idea to deal with this problem was to create an empty list called “delete”.  Inside a for loop that loops from 0 to the length of count, the difference of the number of pictures inside the class and the mean number of images is calculated and rounded. It is then appended to the delete list. The mean value was calculated earlier using np.mean() and has a value of approximately 809. This delete list represents, how many images of each class have to be deleted (if the value is above 0) and how many images have to be added (if the value is below zero) to get the average 809 images. Using a for loop that checks every entry of X_train, the row is then deleted and the value of the delete entry is reduced by one. The entries of X_train and y_train are deleted until the entry in the delete list is 0. This way only the number of images is deleted that is necessary to have the average of 809 images in each class. To make life easier, the X_train list was sorted earlier by class. This was done by using the argsort() function on y_train, which returns the indices of the sorted list. Then a new list was created and the X_train values with the indices in the sorted order were appended. 
I decided to run the program without any measures, that would adjust the number of images in each class. Because I got a validation accuracy of more than the needed 93%, I decided not to adjust the number of images for each class.  
To get a better understanding of the structure of x_train and y_train, I printed the shape of both. This helped me especially for the next step - the pre-processing of the image.
As seen earlier, the number of training images varies a lot between the classes. To increase the number of training images I wanted to add copies of the image set, but adjusted ones. First, I decided to create a copy of inverted images using the cv2.bitwise_not function on the X_train dataset and stack it onto the X_train image set. I decidet to invert the images to get a new set of images, in which the shapes and contours are still the same and the image is not distorted to a point where no sign could be recognised. Next, I defined 4 empty arrays (G, H, L and S). Inside a for loop from 0 to the length of X_train the image is extracted by using the squeeze function. Then it is converted to hls color space by using the cv2.ctvColor() function and saved inside a copy “image_hls”. Furthermore, it was converted to a grayscale image and appended to the G array. The H channel was appended to the Hue array, the Lightness channel to the L array and the Saturation channel to the S array.  Every image inside the arrays was normalized using the formula provided by Udacity (img-128)/128. I wrote a function for this which is applied for the arrays. I chose to convert the images to hls colour space and create different image sets for each channel because the network can learn about hue of the images which can help classify images. For example, the network can easily learn, that a stop sigh is red and a keep right sign is blue. In a grayscale image, this would be harder to detect. For a similar reason I wanted to include the S-channel. The lightness channel is useful, so that the program can predict signs more reliably by detecting, whether for example there is a dark area in the middle of the sign (e.g. the numbers for speed limits) or brighter areas (e.g. yield sign). I chose to use grayscale images also because these images, just like the separate channels have the shape (32,32,3) and not (32,32,3) like RGB images have. I wanted to try it this way because I can imagine that this is easier and faster to compute.   
Afterwards the y_train database is duplicated using the np.tile() function. G, H, S and L are then stacked upon each other and appended to the X_train data. The X_valid images are normalized as well.  To check whether the sizes are equal I printed the sizes of X_train an y_train. Then, some examples are again shown and can be seen in image 3. Before the X_train and y_train data can be used inside the LeNet architecture, it is shuffled by using the shuffle() function. The total number of images for testing is now 278392 instead of the previous 34799. This way the network has more images to train – especially for those classes that were not represented as much as other classes.

The model pipeline consists of a LeNet function just like it was used in the Udacity LeNet lab, but with a few adjustments. Before the LeNet function in defined, the number of epochs and the batch size had to be defined. After testing I got a good feeling of how many epochs I need until the validation accuracy   approaches a certain value and doesn’t increase anymore. I chose 150 Epochs to be on the safe side. In several test runs I was able to get up to 94 to 94 percent accuracy out of the model. I kept the batch size at 128, because it worked well, and I did not have problems with memory usage during testing.   
As mentioned earlier, the model consists of a LeNet architecture like the one in the Udacity LeNet lab. This means, the model has one convolutional layer followed by a first pooling layer, a second convolutional layer and a second pooling layer. Between every convolutional and pooling layer, an activation function is called. After the second pooling layer, the image is flattened and a first fully connected layer is called, followed by another activation function and another fully connected layer, which returns the logits. Let’s go through the layers one by one.  
As mentioned above, the images have a size of 32x32x1, which is the input size for the first convolutional layer. I decided to keep the hyperparameters mu and sigma at 0 and 0.1 since that gave satisfying results. The Output of the first convolutional layer has a depth of 6 (28x28x6), so the tf.zeros() variable must also have the value 6. Using the conv2d function, the convolutional was computed. After this, the layer is activated using a relu() function.  To get rid of unnecessary information, a pooling layer is used, in which the tf.nn.max_pool() function is called. I chose ‘valid’ as padding. This way the image is not padded with zeros. The output size of the pooling layer is 14x14x6. This process I repeated in the second convolutional and pooling layer. Input of the second convolutional layer is the same as the output of the previous layer – 14x14x6. The output depth of this layer is 16. The input size of the second pooling layer is 14x14x16 and the output size is 5x5x16. By calculating 5 times 5 times 16 we get the Output size of the flatten() function which is 400. For the first fully connected layer, this is the input size, while its output has the size of 120. The second fully connected layer takes this output as an input and returns an output the size of 84. The third layer then returns 43 as an output which is necessary because there are 43 different classes inside the databases.  
After the LeNet function was defined, the code for training was created. First, x and y were defined as Tensorflow placeholders (x with the dimensions (none,32,32,1), because no training image is rgb, but grayscale etc.as mentioned earlier). One_hot_y was defined using the tf.one_hot() function with the parameters y and 43, because of the 43 existing classes. Then, the learning rate was defined. I tried setting the learning rate to 0.0005 and to 0.15 but in the end, I decided that 0.001 seems like a good value. To calculate the logits, the LeNet() function described earlier is called with x as an input. The cross entropy is calculated with the tf.nn.softmax_cross_entropy_with_logits() function Afterwards, the loss_operation is calculated using the reduce_mean() function provided by Tensorflow. As an optimizer, I decided to use the AdamOptimizer that was used in the LeNet lab, which uses the learning rate as an input. The training operation is executed by calling optimizer.minimize() ant the correct prediction is defined by tf.equal() and argmax(logits,1) and argmax(one_hot_y,1) as an input. Afterwards the saver is defined as tf.train.saver(). Then a function for evaluating is defined. In this function, the number of images is first calculated by using len() function and the total accuracy is set to 0. In a for loop the total accuracy is calculated. The function returns the total accuracy divided by the number of examples. For training, the sess.run() command is executed, in which the global variables are initialized. Then – as in the evaluation function – the number of examples is defined. Also, 4 different arrays are created, which will store data for evaluation accuracy, training accuracy, the target accuracy and the average accuracy. Those arrays will be filled in every epoch and allow us to plot the progress that the program makes with increasing epochs. At the start of each epoch, the training data set is shuffled. Inside a loop, the X_train set is divided into the batches of the size defined earlier and the sess.run() command us called. The training and evaluating accuracy is calculated by calling the evaluate() function and the training and evaluating dataset as inputs. Then those calculated values are appended to the evaluating and training array. The target accuracy of 0.93 is appended to the target accuracy array and the average accuracy is calculated using the np.mean() function. Then, the validation and training accuracy are plotted for each epoch, along with a plot of the evaluating accuracy (orange), the training accuracy (blue), the target accuracy (red) and the average accuracy (green). The resulting plot is visible in image 4 along with the printed accuracies of the last 3 epochs. As it can be seen, the validation accuracy in the last epoch is 95.3% which I am happy with, so I continues with testing the software on the image set and on images I found on the internet.

To test the network on the image dataset, a session.run() command is executed, where the global variables are again initialized. Then, the LeNet is loaded, that was trained earlier. In a for loop from 0 to the length of the X_test the images are extracted one by one. They are also normalized the same way the training images were normalized earlier. Then the sess.run is called with the logits and the image as a feed_dict input. This returns the prediction for the image. The argmax function returns the indices of the highest value of the prediction, which is appended to an array called classesset. Afterwards I wanted to visualize a few images with the prediction to check, if the program is working well. For that I first created a variable that I set to 5, which represents the number of random images out of the X_test set I want to have visualized. Then, the classes are extracted out of the signnames.csv data using pandas and the red_csv() function. Inside a for loop from 1 to the number of images, I then created a random number using the random.randint() function. A figure is plotted with a number of subplots. Each subplot has an image shown out of the X_test dataset with the random number of the index. As a title for each image, the class is printed. An example can be seen in image 5. It is visible, that every image in this example was predicted correctly.

Next, I wanted to test the program with images from the internet. I decided to download an image of a stop sign, a yield sign, a construction sign, a speed limit sign (120km/h) and a end of all speed limits sign. I chose the speed limit sign, because I thing this could be very difficult to accurately predict the number, since it can easily be mistaken as e.g. 100km/h or an other number, because the round shape with red on the outside is the same for all of those with black numbers in the middle. I chose the construction sign because again it can easily be confused as another sign, especially with a size of 32 by 32, where the shapes in the middle of the sign are very pixelated. I chose the stop sign to see, if the program could confuse it with for example a no entry sign. With the low resolution it could appear very rounded and the white text could be recognised as the white bar that the no entry sign has. The images can be seen in image 6.

Before uploading the images to the jupyter notebook I made sure they have the size 32x32. I saved them in a folder I created called “Pictures”. To load the images into the program I used the glob function that was used in earlier Udacity projects. The images were then displayed in a figure. For prediction of the images, sess.run() was called and the LeNet session was restored. Inside a for loop, each image was loaded from the number of images that were loaded with the glob function earlier. The image is then red using cvw.imread() and converted to rgb colour space using cv2.ctvColor(). Then the image is normalized and just as with the test dataset, the prediction is created using sess.run() and the image as a feed_dict input. The prediction as then printed again as a title to the image. It can be seen in image 7, that every image was classified correctly. Which gives us a accuracy for the 5 images of 100%.

As a last step, the softmax probabilities for those 5 images were displayed. For this, the code for the prediction of the images was taken and modified. After creating the prediction, the class was not chosen with the numpy function argmax(), but with the tf.nn.top_k() function, that was  recommended by Udacity. Since the top 5 probabilities should be displayed for each image, k is set to 5. The probabilities were then printed out. To print these out I was inspired by the knowledge base from Udacity, since I struggled to get a good representation of the softmax probabilities on my own. The resulting softmax probabilities can be seen in image 8. It can be seen that the program had no trouble detecting the stop sign, the probability for it being any other sign is close to 0. The same result applies to the road work sign, the end of all speed limit sign and to the yield sign. For all of those the probability is almost 100 percent. Only for the speed limit sign(120km/h) the probability was only 85 percent. As mentioned earlier I chose this because I thought the classifier might struggle with the number. As it can be seen this is partially true. Although the sign was classified correctly, the program still saw a 10 percent chance, that this image represents a 100km/h speed limit sign and a 3 percent chance that it is a 70 km/h sign. 
